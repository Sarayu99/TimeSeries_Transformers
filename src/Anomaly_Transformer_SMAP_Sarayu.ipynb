{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO8X3cvjpRz8Kdma2Ki2DpL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Anomaly Detection Transformer Code\n","---\n","This notebook runs the anomaly detection code on the SMAP data set <br>\n","\n","Paper link- [here](https://iclr.cc/virtual/2022/spotlight/7024) <br>\n","Code link- [here](https://github.com/thuml/Anomaly-Transformer)\n","\n","Topics in the notebook <br>\n","\n","1. Reading the SMAP data\n","2. DataLoader\n","3. Obtain the data embeddings\n","4. Create the attention layers\n","\n","##### @Notebook author: Sarayu Vyakaranam (svyakara@purdue.edu)\n","\n","---\n","---"],"metadata":{"id":"1ifEISqrcgB7"}},{"cell_type":"markdown","source":["."],"metadata":{"id":"17oSBGhbeegz"}},{"cell_type":"markdown","source":["."],"metadata":{"id":"WTHfu8INefDq"}},{"cell_type":"markdown","source":["---\n","1. Reading the SMAP data\n","---"],"metadata":{"id":"qP20ljcVeDVO"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Os4yCyQBgNTW","executionInfo":{"status":"ok","timestamp":1690916355420,"user_tz":240,"elapsed":295,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ihgIJ3oh4cU","executionInfo":{"status":"ok","timestamp":1690916388293,"user_tz":240,"elapsed":14751,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"c2dde637-b3e7-4d31-d14e-9735054c56e6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["smap_train = np.load('/content/gdrive/MyDrive/Colab Notebooks/SMAP_train.npy')\n","smap_test = np.load('/content/gdrive/MyDrive/Colab Notebooks/SMAP_test.npy')\n","smap_test_label = np.load('/content/gdrive/MyDrive/Colab Notebooks/SMAP_test_label.npy')\n","type(smap_train)\n","print(smap_train)\n","print(len(smap_train))\n","print(len(smap_test))\n","print(len(smap_test_label))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSbC1TWrgae5","executionInfo":{"status":"ok","timestamp":1690916393343,"user_tz":240,"elapsed":1474,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"5a8cabc7-301a-43fa-eae3-d0f1d4c83148"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.999      0.         0.         ... 0.         0.         0.        ]\n"," [0.999      0.         0.         ... 0.         0.         0.        ]\n"," [0.999      0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.98775593 0.         0.         ... 0.         0.         0.        ]\n"," [0.98417906 0.         0.         ... 0.         0.         0.        ]\n"," [0.98417906 0.         0.         ... 0.         0.         0.        ]]\n","135183\n","427617\n","427617\n"]}]},{"cell_type":"code","source":["print(smap_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--KScS4vjKP4","executionInfo":{"status":"ok","timestamp":1690916396159,"user_tz":240,"elapsed":468,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"f73d4393-8d04-4678-e2b6-ea434d8c4b37"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(135183, 25)\n"]}]},{"cell_type":"code","source":["n1 = 100\n","n2 = 100\n","smap_subset = smap_train[:5]\n","print(smap_subset.shape)\n","print(smap_subset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VtHd77ThhoPE","executionInfo":{"status":"ok","timestamp":1690916420905,"user_tz":240,"elapsed":104,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"0ac1dcff-67a2-4898-9b72-f26d69acfe9a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(5, 25)\n","[[0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.   ]\n"," [0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.   ]\n"," [0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.   ]\n"," [0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.   ]\n"," [0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.   ]]\n"]}]},{"cell_type":"code","source":["#check if there is only one non zero per vectors\n","#check sum of vectors"],"metadata":{"id":"A0jGCurxi5aN","executionInfo":{"status":"ok","timestamp":1690906413615,"user_tz":240,"elapsed":2,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["The SMAP input is a sequence of token and value pairs. The token pairs represent the timestamps of the SMAP data, and the value pairs represent the brightness temperature and soil moisture values at those timestamps.\n","\n","\n","The SMAP input is structured as follows:\n","\n","\n","`\n","[\n","  {\n","    \"token\": \"2022-01-01T00:00:00Z\",\n","    \"value\": [0.123, 0.456]\n","  },\n","  {\n","    \"token\": \"2022-01-02T00:00:00Z\",\n","    \"value\": [0.789, 1.234]\n","  },\n","  ...\n","]\n","`\n","\n","\n","The token field is a string that represents the timestamp of the SMAP data. The value field is a list of two numbers that represent the brightness temperature and soil moisture values at that timestamp.\n","\n","The SMAP input is a time series dataset. The timestamps in the SMAP input are evenly spaced, so the SMAP input can be used to train a time series anomaly detection model.\n","\n"],"metadata":{"id":"5MoMf-JDJVqp"}},{"cell_type":"markdown","source":["The input has 25 columns per row because the SMAP dataset was originally collected as a 25-channel time series dataset. Each channel represents a different band of the microwave spectrum. The brightness temperature and soil moisture values are calculated from the values in the 25 channels.\n","\n","The 25 columns in the input are arranged as follows:\n","\n","The first column is the timestamp of the SMAP data.\n","The remaining 24 columns are the brightness temperature and soil moisture values for the 24 channels.\n","The input is preprocessed to remove the timestamp column and to combine the brightness temperature and soil moisture values into a single column. The preprocessed input is then used to train the anomaly detection model."],"metadata":{"id":"T1fQh9kvJ1RX"}},{"cell_type":"markdown","source":["---\n","2. DataLoader\n","---"],"metadata":{"id":"60VK0oobecoe"}},{"cell_type":"code","source":["import torch\n","import os\n","import random\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","import numpy as np\n","import collections\n","import numbers\n","import math\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import pickle"],"metadata":{"id":"H8EDGwb84Dro","executionInfo":{"status":"ok","timestamp":1690916532375,"user_tz":240,"elapsed":5321,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#using only the SMAP class from the 'data_loader.py' file\n","class SMAPSegLoader(object):\n","    def __init__(self, data_path, win_size, step, mode=\"train\"):\n","        #data_path needs to be set as per data location in local computer. For me is is- '/content/gdrive/MyDrive/Colab Notebooks/'\n","        #initilizations\n","        self.mode = mode\n","        self.step = step\n","        self.win_size = win_size\n","        #set scaler\n","        self.scaler = StandardScaler()\n","        #add path to train data\n","        data = np.load(data_path + \"/SMAP_train.npy\")\n","        #pass train data to scaler and transform\n","        self.scaler.fit(data)\n","        data = self.scaler.transform(data)\n","        #repeat same for test data\n","        test_data = np.load(data_path + \"/SMAP_test.npy\")\n","        self.test = self.scaler.transform(test_data)\n","\n","        #store scaled and transformed train and test data\n","        self.train = data\n","        #set validation data also to test data as of now\n","        self.val = self.test\n","        #load the test labels\n","        self.test_labels = np.load(data_path + \"/SMAP_test_label.npy\")\n","        print(\"test:\", self.test.shape)\n","        print(\"train:\", self.train.shape)\n","\n","    def __len__(self):\n","\n","        #returning the length of input considered\n","        if self.mode == \"train\":\n","            return (self.train.shape[0] - self.win_size) // self.step + 1\n","        elif (self.mode == 'val'):\n","            return (self.val.shape[0] - self.win_size) // self.step + 1\n","        elif (self.mode == 'test'):\n","            return (self.test.shape[0] - self.win_size) // self.step + 1\n","        else:\n","            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n","\n","    def __getitem__(self, index):\n","        index = index * self.step\n","        if self.mode == \"train\":\n","            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n","        elif (self.mode == 'val'):\n","            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n","        elif (self.mode == 'test'):\n","            return np.float32(self.test[index:index + self.win_size]), np.float32(\n","                self.test_labels[index:index + self.win_size])\n","        else:\n","            return np.float32(self.test[\n","                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n","                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n","\n","#modified function from 'data_loader.py' because dealing with only SMAP data\n","def get_loader_segment(data_path,batch_size, win_size=100, step=100, mode='train', dataset='SMAP'):\n","    batch = batch_size\n","    if (dataset == 'SMAP'):\n","        dataset = SMAPSegLoader(data_path, win_size, 1, mode)\n","\n","    shuffle = False\n","    if mode == 'train':\n","        shuffle = True\n","    print(batch_size)\n","    data_loader = DataLoader(dataset=dataset,batch_size=batch,shuffle=shuffle,num_workers=0)\n","    return data_loader"],"metadata":{"id":"VldWZzWNepmR","executionInfo":{"status":"ok","timestamp":1690919546006,"user_tz":240,"elapsed":78,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["pip install dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"vxW3DnDbiefO","executionInfo":{"status":"ok","timestamp":1690919100544,"user_tz":240,"elapsed":4688,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"edf8edd6-2653-4eb0-f898-691eea27cf45"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dataloader in /usr/local/lib/python3.10/dist-packages (2.0)\n"]}]},{"cell_type":"code","source":["path = '/content/gdrive/MyDrive/Colab Notebooks/'\n","dataset_name = 'SMAP'\n","train_loader = get_loader_segment(data_path=path, batch_size=8, win_size=20,mode='train',dataset=dataset_name)\n","vali_loader = get_loader_segment(data_path=path, batch_size=8, win_size=20,mode='val',dataset=dataset_name)\n","test_loader = get_loader_segment(data_path=path, batch_size=8, win_size=20,mode='test',dataset=dataset_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3XbAPtBe2MI","executionInfo":{"status":"ok","timestamp":1690921441578,"user_tz":240,"elapsed":1004,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"122fa58c-e7ae-4798-edc6-8ce77598ac65"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["test: (427617, 25)\n","train: (135183, 25)\n","8\n","test: (427617, 25)\n","train: (135183, 25)\n","8\n","test: (427617, 25)\n","train: (135183, 25)\n","8\n"]}]},{"cell_type":"code","source":["#understanding the data loader better\n","\n","for batch,sigma in train_loader:\n","  #note each batch is 8 observations, each observation is 20X25\n","  print(len(batch))\n","  print(len(sigma))\n","  print(batch)\n","  print('-------')\n","  #note each sigma is 1X20, so for the total batch there are 8X20 rows\n","  print(sigma)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdEJDh8_CZG5","executionInfo":{"status":"ok","timestamp":1690925962997,"user_tz":240,"elapsed":209,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"c4a5031f-9c8c-4d0f-fb6c-d43c06b7364e"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n","8\n","tensor([[[-0.0742, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.0776, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.1485, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [-0.1815, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.1688, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.1745, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]],\n","\n","        [[ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5050, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]],\n","\n","        [[-0.4330, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.4321, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.4313, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [-0.4180, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.4172, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-0.4163, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]],\n","\n","        ...,\n","\n","        [[-1.0244, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0244, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0244, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [-1.0242, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0242, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0242, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]],\n","\n","        [[ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [ 1.5038, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]],\n","\n","        [[-1.0059, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0059, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0059, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         ...,\n","         [-1.0059, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0057, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038],\n","         [-1.0057, -0.1684, -0.0734,  ..., -0.1672, -0.0067, -0.0038]]])\n","-------\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"code","source":["print(type(train_loader))\n","print(dir(train_loader))\n","length_train = 0\n","length_test = 0\n","length_vali = 0\n","c = 0\n","\n","#training data\n","for nth_batch, (batch,_) in enumerate(train_loader):\n","  #print(len(batch))\n","  c += 1\n","  length_train += len(batch)\n","  if c == 1:\n","    print('the first batch')\n","    print(f'length of each batch is (window size)- {len(batch[0])}')\n","    #each tensor below had 20X25 2D form where 20 is the window size, 25 is fixed in the initial data\n","    print(batch[0])\n","    print(batch[1])\n","    print(batch[2])\n","    print(batch[3])\n","    print(batch[4])\n","    print(batch[5])\n","    print(batch[6])\n","    print(batch[7])\n","    print(batch[7])\n","print(length_train)\n","print(f'the number of rows in training data = {length_train}')\n","print(f'the number of batches in training data = {nth_batch}')\n","#test data\n","for nth_batch, (batch,_) in enumerate(test_loader):\n","  #print(len(batch))\n","  length_test += len(batch)\n","print(f'the number of rows in testing data = {length_test}')\n","print(f'the number of batches in testing data = {nth_batch}')\n","\n","#validation data\n","for nth_batch, (batch,_) in enumerate(vali_loader):\n","  #print(len(batch))\n","  length_vali += len(batch)\n","print(f'the number of rows in validation data = {length_vali}')\n","print(f'the number of batches in validation data = {nth_batch}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUjGTjrhiQIN","executionInfo":{"status":"ok","timestamp":1690926286902,"user_tz":240,"elapsed":19981,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"c834091a-639b-400b-a648-a4010a5ec29d"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.utils.data.dataloader.DataLoader'>\n","['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_is_protocol', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'pin_memory_device', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n","the first batch\n","length of each batch is (window size)- 20\n","tensor([[ 1.3808, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4143, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3856, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3952, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3999, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4095, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4429, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4286, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4238, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4334, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4668, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4811, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4525, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4716, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4907, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4620, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4668, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4811, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4572, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.4620, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[ 1.3865, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3723, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3675, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3581, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3581, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3581, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3439, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.3344, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 1.1029, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 0.7740, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 0.5449, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 0.3076, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [ 0.1036, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.0488, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.1824, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.3157, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.4165, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.5218, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.5949, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.6997, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-0.8770, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00,  5.9398e+00, -7.3379e-02,  6.7114e+00, -1.6093e-02,\n","          2.6092e+00, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01,  9.5472e+00,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00,  6.8486e+00,  6.8949e+00, -1.0474e-01,\n","         -9.4221e-03,  5.8073e+00,  5.9795e+00, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00,  6.8486e+00,  6.8949e+00, -1.0474e-01,\n","         -9.4221e-03,  5.8073e+00,  5.9795e+00, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00,  5.9398e+00, -7.3379e-02,  6.7114e+00, -1.6093e-02,\n","          2.6092e+00, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03]])\n","tensor([[-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0312, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0279, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0277, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038],\n","        [-1.0277, -0.1684, -0.0734, -0.1490, -0.0161, -0.3833, -0.3296, -0.0331,\n","         -0.0192, -0.0219, -0.0027, -0.0331, -0.0194, -0.0248, -0.0248, -0.0054,\n","          0.0000, -0.1460, -0.1450, -0.1047, -0.0094, -0.1722, -0.1672, -0.0067,\n","         -0.0038]])\n","tensor([[-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00,  6.8486e+00,  6.8949e+00, -1.0474e-01,\n","         -9.4221e-03,  5.8073e+00,  5.9795e+00, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00,  5.9398e+00, -7.3379e-02,  6.7114e+00, -1.6093e-02,\n","          2.6092e+00, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03]])\n","tensor([[-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00,  6.8486e+00,  6.8949e+00, -1.0474e-01,\n","         -9.4221e-03,  5.8073e+00,  5.9795e+00, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","          2.6092e+00,  3.0337e+00, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00,  5.9398e+00, -7.3379e-02,  6.7114e+00, -1.6093e-02,\n","          2.6092e+00, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03],\n","        [-1.0312e+00, -1.6836e-01, -7.3379e-02, -1.4900e-01, -1.6093e-02,\n","         -3.8326e-01, -3.2963e-01, -3.3106e-02, -1.9236e-02, -2.1933e-02,\n","         -2.7198e-03, -3.3106e-02, -1.9427e-02, -2.4786e-02, -2.4786e-02,\n","         -5.4397e-03,  0.0000e+00, -1.4602e-01, -1.4503e-01, -1.0474e-01,\n","         -9.4221e-03, -1.7220e-01, -1.6724e-01, -6.6623e-03, -3.8464e-03]])\n","135164\n","the number of rows in training data = 135164\n","the number of batches in training data = 16895\n","the number of rows in testing data = 427598\n","the number of batches in testing data = 53449\n","the number of rows in validation data = 427598\n","the number of batches in validation data = 53449\n"]}]},{"cell_type":"markdown","source":["Note: obtaining slightly fewer number than total input length, not sure why"],"metadata":{"id":"wUeQdwEbt7pF"}},{"cell_type":"markdown","source":["---\n","3. Obtain the data embeddings\n","---"],"metadata":{"id":"FRQzTq6c1z1d"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import weight_norm\n","import math"],"metadata":{"id":"z0aEsMcFqEDq","executionInfo":{"status":"ok","timestamp":1690922620190,"user_tz":240,"elapsed":89,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        # Compute the positional encodings once in log space.\n","        super(PositionalEmbedding, self).__init__()\n","\n","        #d_model is the dimension of the embedding. This means that each token in the embedding layer will have a representation of d_model dimensions.\n","        #The d_model dimension is a hyperparameter that can be tuned to improve the performance of the model\n","        #its been set to 128 in the paper\n","        pe = torch.zeros(max_len, d_model).float()\n","        #setting require_grad to TRUE allows the gradient of the positional encoding matrix to be calculated during backpropagation.\n","        #This allows the model to learn the optimal positional encoding matrix for the given task.\n","        #not sure why its been set to False for now\n","        pe.require_grad = False\n","\n","        #The torch.arange() function creates a sequence of numbers from 0 to max_len.\n","        #The float() function converts the sequence of numbers to floating point numbers\n","        #The unsqueeze() function adds a new dimension to the tensor.\n","        position = torch.arange(0, max_len).float().unsqueeze(1)\n","        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n","\n","        #The div_term tensor is created by first creating a sequence of numbers from 0 to d_model in steps of 2.\n","        #The sequence of numbers is then converted to floating point numbers and multiplied by a negative exponential function.\n","        #The negative exponential function is used to create a decaying function that decreases as the position increases.\n","        #The div_term tensor is then used to create the positional encoding matrix.\n","        #The positional encoding matrix is a 2D tensor that has the same shape as the input sequences.\n","        #The values in the positional encoding matrix are calculated by taking the div_term tensor and adding it to a sinusoid function.\n","        #The sinusoid function is used to create a wave-like pattern that represents the position of the token in the sequence.\n","        #The positional encoding matrix is then added to the input sequences before they are passed to the model.\n","        #The positional encoding helps the model learn the temporal relationships between the input sequences.\n","        #This is important for tasks such as machine translation and natural language understanding,\n","        #where the order of the words in the sequence is important.\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    #The forward function is not called when the model is initialized.\n","    #The forward function is only called when the model is used to make predictions or calculate the loss.\n","    def forward(self, x):\n","        return self.pe[:, :x.size(1)]\n","\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, c_in, d_model):\n","        super(TokenEmbedding, self).__init__()\n","        padding = 1 if torch.__version__ >= '1.5.0' else 2\n","        #1D convolutional layers are used to extract features from sequences.\n","        #In the case of the TokenEmbedding class, the 1D convolutional layer is used to extract features from the token sequences.\n","        #The 1D convolutional layer takes the token sequences as input and outputs a sequence of features.\n","        #The number of features in the output sequence depends on the number of filters in the 1D convolutional layer.\n","        #The filters in the 1D convolutional layer are learned during training.\n","\n","        #kernel_size specifies the size of th ekernel in the 1D convolution\n","        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n","\n","        #self.modules is a dictionary that contains all of the submodules of the TokenEmbedding class.\n","        #The TokenEmbedding class has two submodules: positional_encoding and conv1d.\n","        #The positional_encoding submodule is a positional encoding layer that is used to add positional information to the input sequences.\n","        #The conv1d submodule is a 1D convolutional layer that is used to extract features from the token sequences.\n","        #The self.modules attribute is used to access the submodules of the TokenEmbedding class.\n","        #For example, the following code snippet can be used to access the positional_encoding submodule:\n","        #     model = TokenEmbedding()\n","        #     positional_encoding = model.modules['positional_encoding']\n","        for m in self.modules():\n","            #if the token embedding exists for the submodule\n","            if isinstance(m, nn.Conv1d):\n","                #The nn.init.kaiming_normal_ function in PyTorch is used to initialize the weights of a neural network layer.\n","                #The function initializes the weights using a normal distribution with a variance that is inversely proportional to the fan-in of the layer.\n","                #It is often used in conjunction with ReLU activation functions.\n","                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","\n","    def forward(self, x):\n","        #forward does 3 things-\n","        #1. Permutes the input tensor x so that the batch dimension is the first dimension, the sequence dimension is the second dimension, and the feature dimension is the third dimension.\n","        #2. Pass the permuted tensor x to the tokenConv layer.\n","        #3. Transposes the output tensor of the tokenConv layer so that the sequence dimension is the first dimension and the feature dimension is the second dimension.\n","        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n","        return x\n","\n","\n","class DataEmbedding(nn.Module):\n","    #the c_in taken here is nothing but enc_in size- here input_c = 25 and output_c = 25 (see shell script, these are parameters taken by the parser)\n","    #note: see build_model in the main.py file- the model is initilaized using the input_c = 25 and output_c = 25\n","    def __init__(self, c_in, d_model, dropout=0.0):\n","        super(DataEmbedding, self).__init__()\n","\n","        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n","        self.position_embedding = PositionalEmbedding(d_model=d_model)\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        x = self.value_embedding(x) + self.position_embedding(x)\n","        return self.dropout(x)\n"],"metadata":{"id":"qMH56hmR15Pk","executionInfo":{"status":"ok","timestamp":1690925618682,"user_tz":240,"elapsed":82,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["# Encoding for the train\n","enc_in = 25 #embedding size\n","embedding = DataEmbedding(c_in=enc_in, d_model=512, dropout=0.0)\n","print(embedding)\n","print(type(embedding))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U52-ldxS2JAK","executionInfo":{"status":"ok","timestamp":1690926617896,"user_tz":240,"elapsed":83,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"c9dd3d78-d153-405e-b4c0-f1d51d3485e2"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["DataEmbedding(\n","  (value_embedding): TokenEmbedding(\n","    (tokenConv): Conv1d(25, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n","  )\n","  (position_embedding): PositionalEmbedding()\n","  (dropout): Dropout(p=0.0, inplace=False)\n",")\n","<class '__main__.DataEmbedding'>\n"]}]},{"cell_type":"code","source":["for nth_batch, (batch,sigma) in enumerate(train_loader):\n","  batch_embedding = embedding(batch)\n","  print(batch_embedding.shape)\n","  print(len(batch_embedding))\n","  print(batch_embedding[0].shape)\n","  print(len(batch_embedding[0]))\n","  #'the size of each embedding is 20X512 which is win_size X d_model\n","  #print a sample embedding\n","  print(batch_embedding[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5z32JkJBmjf","executionInfo":{"status":"ok","timestamp":1690928218801,"user_tz":240,"elapsed":79,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}},"outputId":"ab837fb0-35c0-45cf-f5c5-80639ec3565e"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 20, 512])\n","8\n","torch.Size([20, 512])\n","20\n","tensor([[ 9.9732e-02,  1.2029e+00, -3.8141e-01,  ...,  9.6707e-01,\n","         -3.4434e-01,  9.1254e-01],\n","        [ 1.0001e+00,  8.2361e-01,  2.9003e-01,  ...,  9.6176e-01,\n","         -3.9440e-01,  8.2551e-01],\n","        [ 1.0665e+00, -1.2842e-01,  4.0459e-01,  ...,  9.6126e-01,\n","         -3.9475e-01,  8.2282e-01],\n","        ...,\n","        [-8.1695e-01,  7.7838e-04, -1.1457e+00,  ...,  9.6404e-01,\n","         -3.9339e-01,  8.3413e-01],\n","        [-6.1089e-01,  9.7003e-01, -1.5262e+00,  ...,  9.6435e-01,\n","         -4.1689e-01,  8.0486e-01],\n","        [ 2.9268e-01,  1.2156e+00, -9.8844e-01,  ...,  9.7389e-01,\n","         -4.0166e-01,  8.6412e-01]], grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"markdown","source":["---\n","4. Create the attention layers\n","---"],"metadata":{"id":"gb0J4sSJLY61"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import math\n","from math import sqrt\n","import os\n"],"metadata":{"id":"9emrqxICCAeR","executionInfo":{"status":"ok","timestamp":1690928268851,"user_tz":240,"elapsed":380,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["#used to create a mask that prevents the model from attending to future tokens when predicting the current token.\n","#The mask is a triangular matrix where the diagonal elements are all 1 and the off-diagonal elements are all 0\n","#This ensures that the model only learns dependencies between the current token and the tokens that have already been seen\n","class TriangularCausalMask():\n","    def __init__(self, B, L, device=\"cpu\"):\n","        #define the mask shape- to be B=batch size, L=sequence length, notice L is being passed to the mask function\n","        mask_shape = [B, 1, L, L]\n","        with torch.no_grad():\n","            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n","\n","    @property\n","    def mask(self):\n","        return self._mask\n","\n","\n","class AnomalyAttention(nn.Module):\n","    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n","        super(AnomalyAttention, self).__init__()\n","        self.scale = scale\n","        self.mask_flag = mask_flag\n","        self.output_attention = output_attention\n","        self.dropout = nn.Dropout(attention_dropout)\n","        window_size = win_size\n","        #tensor that stores the distances between all pairs of sequences in the dataset.\n","        #The distances are calculated using the Euclidean distance metric\n","        self.distances = torch.zeros((window_size, window_size)).cuda()\n","        for i in range(window_size):\n","            for j in range(window_size):\n","                self.distances[i][j] = abs(i - j)\n","\n","    def forward(self, queries, keys, values, sigma, attn_mask):\n","       #B : The batch size= 2 in the paer I think., L : The sequence length=10 i think,\n","       #H : The hidden size of the transformer=128, E : The embedding size=25.\n","        B, L, H, E = queries.shape\n","        _, S, _, D = values.shape\n","        #scaling to prevent attention weights from becoming too large\n","        scale = self.scale or 1. / sqrt(E)\n","\n","        #the shape of scores is B L H S (Note: I think S denotes the same thing as L, both represent sequence length)\n","        #The scores tensor contains the attention scores between the queries and keys tensors\n","        #teh code below specifies the tensor contraction to be performed\n","        #blhe: The first part of the string specifies the shape of the first tensor, which is the queries tensor. The blhe shape indicates that the queries tensor has four dimensions: batch size (b), sequence length (l), hidden size (h), and embedding size (e).\n","        #bshe: The second part of the string specifies the shape of the second tensor, which is the keys tensor. The bshe shape indicates that the keys tensor has the same shape as the queries tensor.\n","        #->: The -> symbol indicates that the tensor contractions should be performed.\n","        #bhls: The third part of the string specifies the shape of the output tensor, which is the scores tensor. The bhls shape indicates that the scores tensor has four dimensions: batch size (b), sequence length (l), hidden size (h), and sequence length (l) again.\n","        #the following tensor contractions are perfomed-\n","        #1. The queries tensor is multiplied by the keys tensor.\n","        #2. The product of the queries tensor and the keys tensor is summed over the embedding size dimension.\n","        #3. The sum of the products is then reshaped to have the shape of the scores tensor.\n","\n","        #note: S and E need not be same\n","        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n","\n","\n","        #apply mask if needed\n","        if self.mask_flag:\n","            if attn_mask is None:\n","                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n","            scores.masked_fill_(attn_mask.mask, -np.inf)\n","\n","        #scale\n","        attn = scale * scores\n","\n","        sigma = sigma.transpose(1, 2)  # B L H ->  B H L\n","        window_size = attn.shape[-1]\n","        sigma = torch.sigmoid(sigma * 5) + 1e-5\n","        sigma = torch.pow(3, sigma) - 1\n","        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)  # B H L L #I think this is the same dimension as the scores\n","        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda()\n","        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))\n","\n","        series = self.dropout(torch.softmax(attn, dim=-1))\n","        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n","\n","        if self.output_attention:\n","            return (V.contiguous(), series, prior, sigma)\n","        else:\n","            return (V.contiguous(), None)\n","\n","\n","class AttentionLayer(nn.Module):\n","    def __init__(self, attention, d_model, n_heads, d_keys=None,\n","                 d_values=None):\n","        super(AttentionLayer, self).__init__()\n","\n","        d_keys = d_keys or (d_model // n_heads)\n","        d_values = d_values or (d_model // n_heads)\n","        self.norm = nn.LayerNorm(d_model)\n","        self.inner_attention = attention\n","        self.query_projection = nn.Linear(d_model,\n","                                          d_keys * n_heads)\n","        self.key_projection = nn.Linear(d_model,\n","                                        d_keys * n_heads)\n","        self.value_projection = nn.Linear(d_model,\n","                                          d_values * n_heads)\n","        self.sigma_projection = nn.Linear(d_model,\n","                                          n_heads)\n","        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n","\n","        self.n_heads = n_heads\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L, _ = queries.shape\n","        _, S, _ = keys.shape\n","        H = self.n_heads\n","        x = queries\n","        queries = self.query_projection(queries).view(B, L, H, -1)\n","        keys = self.key_projection(keys).view(B, S, H, -1)\n","        values = self.value_projection(values).view(B, S, H, -1)\n","        sigma = self.sigma_projection(x).view(B, L, H)\n","\n","        out, series, prior, sigma = self.inner_attention(\n","            queries,\n","            keys,\n","            values,\n","            sigma,\n","            attn_mask\n","        )\n","        out = out.view(B, L, -1)\n","\n","        return self.out_projection(out), series, prior, sigma\n"],"metadata":{"id":"2t-5hV7eLcQq","executionInfo":{"status":"ok","timestamp":1690930397010,"user_tz":240,"elapsed":101,"user":{"displayName":"Sarayu Vyakaranam","userId":"13294961587060716193"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B_ExCLdKLjY0"},"execution_count":null,"outputs":[]}]}